



## glm4

- https://huggingface.co/THUDM/glm-4-9b-chat
- https://huggingface.co/THUDM/glm-4-9b-chat-1m




```
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# GLM-4-9B-Chat-1M
# max_model_len, tp_size = 1048576, 4

# GLM-4-9B-Chat
# 如果遇见 OOM 现象，建议减少max_model_len，或者增加tp_size
max_model_len, tp_size = 131072, 1

model_name = "THUDM/glm-4-9b-chat"
prompt = [{"role": "user", "content": "你好"}]

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
llm = LLM(
    model=model_name,
    tensor_parallel_size=tp_size,
    max_model_len=max_model_len,
    trust_remote_code=True,
    enforce_eager=True,
    # GLM-4-9B-Chat-1M 如果遇见 OOM 现象，建议开启下述参数
    # enable_chunked_prefill=True,
    # max_num_batched_tokens=8192
)
stop_token_ids = [151329, 151336, 151338]
sampling_params = SamplingParams(temperature=0.95, max_tokens=1024, stop_token_ids=stop_token_ids)

inputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)
outputs = llm.generate(prompts=inputs, sampling_params=sampling_params)

print(outputs[0].outputs[0].text)

```


```



python3 -m vllm.entrypoints.openai.api_server \
--port 9009 \
--gpu-memory-utilization 0.95 \
--dtype auto \
--model /workspace/models/Qwen1.5-7B-Chat \
--served-model-name glm-4-9b-chat \
--max-num-seqs 128 \
--max-model-len 131072 \
--tensor-parallel-size 1 \
--enforce-eager \
--enable-chunked-prefill \
--max-num-batched-tokens 10000 \
--kv-cache-dtype fp8



```




## qwen2.5

- https://huggingface.co/Qwen/Qwen2.5-7B-Instruct



```
    {
        "architectures": [
            "Qwen2ForCausalLM"
        ],
        // ...
        "vocab_size": 152064,

        // adding the following snippets
        "rope_scaling": {
            "factor": 4.0,
            "original_max_position_embeddings": 32768,
            "type": "yarn"
        }
    }

```


目前，vLLM 仅支持静态 YARN，这意味着无论输入长度如何，缩放因子都保持不变，这可能会影响较短文本的性能。我们建议仅在需要处理长上下文时添加rope_scaling配置。

