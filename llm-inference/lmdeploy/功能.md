





https://lmdeploy.readthedocs.io/en/latest/inference/turbomind_config.html


## prefix caching switch




## kv quantization and inference switch




## long context switch







