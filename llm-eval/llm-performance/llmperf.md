

- https://github.com/ray-project/llmperf-leaderboard
- https://github.com/ray-project/llmperf
- 指标定义：https://github.com/ray-project/llmperf/blob/main/src/llmperf/common_metrics.py


- 可复现的语言大模型推理性能指标: https://zhuanlan.zhihu.com/p/667612787
- 原文：https://www.anyscale.com/blog/reproducible-performance-metrics-for-llm-inference


## 通用指标

- 每分钟完成的请求数

- 首词元时间（TTFT）

在流式应用中，TTFT指的是LLM返回第一个词元前所需的时间。

- 词元间时延（ITL）
词元间时延指的是连续输出词元之间的平均时间。 将TTFT纳入词元间时延的计算。


- 端到端时延
端到端时延应该大致等于词元的平均输出长度乘以词元间时延。


## 特定实例的额外指标

### 配置


同一模型通常由于配置不同，导致在时延、成本和吞吐量之间出现不同权衡。例如，在p4de实例（AWS发布了的EC2 GPU执行实例P4de）上运行的CodeLlama-34B模型可以配置为8个副本，每个副本有1个GPU，也可以配置为4个副本，每个副本有2个GPU，或者配置为2个副本，每个副本有4个GPU，甚至还可以配置为1个副本，拥有全部8个GPU。你还可以为流水并行或张量并行配置多个GPU。


每种配置都有不同特性：每个副本有一个GPU的情况可能拥有最低的TTFT（因为有8个“队列”等待输入），而一个副本有8个GPU的情况可能具备最大的吞吐量（因为有更多的批处理内存，且实际上有8倍的内存带宽）。


每种配置都会导致不同的基准测试结果。


### 输出词元吞吐量


总生成词元吞吐量，这便于比较成本。


##  考虑到但没有包含的测量标准

### 预加载时间

由于预加载时间只能通过对输入大小的首词元的回归来间接测算，因此我们在这一轮基准测试中没有加入这一指标。


在250个词元输入和800个词元输入之间，输入词元与TTFT之间似乎并不存在明显的关系，且因其他原因导致的TTFT的随机噪声“掩盖（swamped）”了这一关系。

实际上，我们尝试过使用回归分析来估计这一关系，通过比较550个输入词元和3500个输入词元的输出，并估算梯度，我们发现每增加一个输入词元会增加0.3-0.7毫秒的端到端时间，

相比之下，每增加一个输出词元会增加30-60毫秒的端到端时间。因此，输入词元对端到端时延的影响约为输出词元的1%。


### 总吞吐量（包括输入和生成的词元）





## 输入选择

### 输入大小
	
平均输入长度：550个词元（标准差为150个词元）
平均输出长度：150个词元（标准差为20个词元）

为简化问题，我们假设输入和输出都服从正态分布。在未来的工作中，我们将考虑Poisson分布等更具代表性的分布，因为这类分布在建模词元分布方面性质更佳。


### 并发请求


一个关键特征是同时发出的请求数量。显然，更多的并发请求会使固定资源集的输出速度变慢。在测试中，我们已经将5作为关键数字进行了标准化。


## 分析


ITL和TTFT都足够小，不会成为主要问题，因为人类每秒大约阅读5个词元，即便方案中最慢的速度也是人类的6倍之多，所以它们之间没有显著的差异。


100个输入词元与单个输出词元对时延的影响大致相当。若想提升速度，减少输出要比减少输入更有效。


