

- 吞吐率 Throughput 是指什么：https://www.zhihu.com/question/596311688
- [大模型场景下训练和推理性能指标名词解释](https://bbs.huaweicloud.com/blogs/416186)
- [大模型性能概念明确](https://www.hiascend.com/document/detail/zh/Pytorch/60RC1/ptmoddevg/trainingmigrguide/performance_tuning_0004.html)

## 4090 


|模型           | 数据集      | GPU型号 | 卡数 | 训练方式              | batch size | 每秒处理样本（sample/s） | 训练时长    | 显存占用(MB)             |
| ------------ | -------- | ----- | -- | ----------------- | ---------- | ---------------- | ------- | -------------------- |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | 全量（Zero3-offload） | 1          | 0.085            | 3:35:20 | 15075.9375           |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | 全量（Zero3-offload） | 2          | 0.215            | 1:25:26 | 15961.9375           |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | 全量（Zero3-offload） | 4          | 0.342            | 53:36   | 16925.9375           |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | 全量（Zero3-offload） | 8          | 0.653            | 28:04   | 21845.9375           |
| baichuan2-7b | xxx_1.1k | 4090  | 2  | 全量（Zero3-offload） | 4          | 0.909            | 20:10   | 19869.937519871.9375 |
| baichuan2-7b | xxx_1.1k | 4090  | 2  | 全量（Zero3-offload） | 8          | 1.554            | 11:47   | 23397.937524469.9375 |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | lora              | 1          | 1.138            | 16:06   | 21399.9375           |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | lora              | 2          | 1.341            | 13:40   | 22411.9375           |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | lora              | 4          | -               | -      | OOM                  |
| baichuan2-7b | xxx_1.1k | 4090  | 2  | lora              | 2          | 2.609            | 07:01   | 24331.937524187.9375 |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | qlora（paged）      | 1          | 1.933            | 09:29   | 16363.9375           |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | qlora（paged）      | 2          | 1.804            | 10:09   | 18327.9375           |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | qlora（paged）      | 4          | 1.576            | 11:37   | 24119.9375           |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | qlora（paged）      | 8          | -               | -      | OOM                  |
| baichuan2-7b | xxx_1.1k | 4090  | 2  | qlora（paged）      | 2          | 3.497            | 05:14   | 19035.937519069.9375|

-   使用ZeRO3-offload 内存至少128G，推荐256G
-   qlora使用优化器使用paged_adamw_32bit进行内存优化，避免GPU 偶尔的OOM




| 模型           | 数据集      | GPU型号 | 卡数 | 训练方式              | batch size | 每秒处理样本（sample/s） | 每秒处理词元数（token/s） | 训练时长  | 显存占用(MB)   |
| ------------ | -------- | ----- | -- | ----------------- | ---------- | ---------------- | ---------------- | ----- | ---------- |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | 全量（Zero3-offload） | 8          | 0.653            | 474.11           | 28:04 | 21845.9375 |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | lora              | 2          | 1.341            | 991.86           | 13:40 | 22411.9375 |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | qlora（paged）      | 4          | 1.576            | 1169.29          | 11:37 | 24119.9375 |




**数据集**： xxx_1.1k，1100条训练集，共797459词元。
**服务器配置**：单卡 RTX 4090 24G + 256G内存（至少128G）
**模型训练** **配置：**

全量、LoRA、QLoRA 配置：

-   max_seq_length：1024
-   gradient_checkpointing：true
-   epoch：1

LoRA、QLoRA 配置

-   lora_rank：64
-   lora_alpha：16
-   lora_dropout：0.05

QLoRA 配置：

-   optim：paged_adamw_32bit





GPU型号性能对比

| 模型           | 数据集      | GPU型号         | 卡数 | 训练方式              | batch size | 每秒处理样本（sample/s） | 训练时长  | 显存占用(MB)             |
| ------------ | -------- | ------------- | -- | ----------------- | ---------- | ---------------- | ----- | -------------------- |
| baichuan2-7b | xxx_1.1k | 3090(QPI/UPI) | 2  | 全量（Zero3-offload） | 4          | 0.722            | 25:22 | 2189821892           |
| baichuan2-7b | xxx_1.1k | 4090(QPI/UPI) | 2  | 全量（Zero3-offload） | 4          | 0.909            | 20:10 | 19869.937519871.9375 |
| baichuan2-7b | xxx_1.1k | A800(pcie)    | 2  | 全量（Zero3-offload） | 4          | 1.136            | 16:08 | 1955620092           |
| baichuan2-7b | xxx_1.1k | H800(nvlink)  | 2  | 全量（Zero3-offload） | 4          | 2.51             | 07:18 | 1986919859           |
| baichuan2-7b | xxx_1.1k | 3090          | 1  | lora(int8)        | 2          | 0.77             | 23:47 | 20822MiB             |
| baichuan2-7b | xxx_1.1k | 4090          | 1  | lora(int8)        | 2          | 1.341            | 13:40 | 22411.9375           |
| baichuan2-7b | xxx_1.1k | A800          | 1  | **lora(int8)**    | 2          | 0.857            | 21:23 | 22670MiB             |
| baichuan2-7b | xxx_1.1k | **A800**      | 1  | **lora(fp16)**    | 2          | 2.203            | 08:19 | 50206MiB             |
| baichuan2-7b | xxx_1.1k | **H800**      | 1  | lora(fp16)        | 2          | 4.468            | 04:06 | 55328MiB             |
| baichuan2-7b | xxx_1.1k | A800          | 1  | qlora(nf4)        | 2          | 2.018            | 09:05 | 16978MiB             |
| baichuan2-7b | xxx_1.1k | H800          | 1  | qlora(nf4)        | 2          | 3.465            | 05:17 | 18173MiB             |
| chatglm3-6b  | xxx_1.1k | 3090          | 1  | qlora             | 2          | 0.984            | 18.37 | 11300                |
| chatglm3-6b  | xxx_1.1k | 4090          | 1  | qlora             | 2          | 1.727            | 10:36 | 11722                |
| chatglm3-6b  | xxx_1.1k | A800          | 1  | qlora             | 2          | 1.866            | 09:49 | 12230                |
| chatglm3-6b  | xxx_1.1k | H800          | 1  | qlora             | 2          | 2.768            | 06:37 | 12056                |


注意：
- H800基于cuda-11.8执行qlora/lora训练会报错：Error named symbol not found at line 74 in file /workspace/train-env/bitsandbytes/csrc/ops.cu ，解决办法：升级到cuda-12.x
- H800基于Lora训练，使用llm.int8加载原始模型暂不支持。


不同模型性能对比

| 模型           | 数据集      | GPU型号 | 卡数 | 训练方式 | batch size | 每秒处理样本（sample/s） | 训练时长       | 显存占用(MB)           |
| ------------ | -------- | ----- | -- | ---- | ---------- | ---------------- | ---------- | ------------------ |
| baichuan-7b  | xxx_1.1k | 4090  | 1  | lora | 2          | 1.15             | 15:56      | 18279.9375         |
| baichuan2-7b | xxx_1.1k | 4090  | 1  | lora | 2          | 1.341            | 13:40      | 22411.9375         |
| chatglm3-6b  | xxx_1.1k | 4090  | 1  | lora | 2          | 1.3031.505       | 14:0412:10 | 17025.937516636MiB |
| bloom-2b6    | xxx_1.1k | 4090  | 1  | lora | 2          | 1.951            | 09:23      | 8719.9375          |


## Ascend 910B

### mindformers

- 百川2：https://gitee.com/mindspore/mindformers/blob/dev/research/baichuan2/baichuan2.md#%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD






## 吞吐量计算

- ds train_micro_batch_size_per_gpu=2 vs hf per_device_train_batch_size=8
- ds gradient_accumulation_steps=8 vs hf gradient_accumulation_steps=1
- ds train_batch_size=128 vs hf train_batch_size (calculated)=64






## MFU / HFU




Model FLOPs Utilization (MFU) and Hardware FLOPS Utilization (HFU) 

根据测量的吞吐量和已知的计算 FLOPs 来估计训练期间使用的硬件 FLOPs 的百分比。



MFU 根据模型的单个前向/后向传递所需的浮点运算计算利用率，并且不考虑其他实现细节（例如:激活检查点）所需的额外计算。因此，MFU 独立于实现和硬件。

HFU 尝试捕获硬件上前向/后向传递期间发生的实际浮点运算。虽然它是对硬件利用率的更准确测量，但它不太通用，并且很难在各种硬件和实现细节之间进行比较。



MPT 训练基准测试：https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/benchmarking/README.md




